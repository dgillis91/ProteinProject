{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of \"Similar\" Sequences Using The Recurrent Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I'm doing a thing to generate similar protein sequences, eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Try:\n",
    "# ! Break into training files\n",
    "# ! with different seq lens. \n",
    "# ! Start with longer sequences.\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding\n",
    "from keras.layers import Input, Lambda, LSTM, RepeatVector\n",
    "from keras.layers import Flatten, TimeDistributed, Layer, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Activation\n",
    "from keras.layers.advanced_activations import ELU\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import objectives\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model, get_file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We are working with a file that contains\n",
    "sequences. Each sequence is on a new line.\n",
    "Because of the way the sequences are pulled,\n",
    "there are additional \"-\" characters used for\n",
    "alignment. We will strip these out, and pad\n",
    "the front of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "url = 'https://raw.githubusercontent.com/badriadhikari'\n",
    "deepcon_path = 'DEEPCON/master/deepcon-covariance/test'\n",
    "file = '16pkA0.aln'\n",
    "filepath = os.path.join(url, deepcon_path, file)\n",
    "#file = get_file(file, filepath)\n",
    "file = '/home/das-hund/PycharmProjects/autoencoders/data/pdb_seqres.txt'\n",
    "\n",
    "with open(file, 'r') as sequence_file:\n",
    "    sequences = sequence_file.read() \\\n",
    "                             .replace('-', '') \\\n",
    "                             .split('\\n')\n",
    "\n",
    "sequence_count = len(sequences)\n",
    "print(f'[+] {sequence_count} Sequences in {file}')\n",
    "print(f'[+] Subset of Sequences:')\n",
    "print('\\n\\n'.join(sequences[:5]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Appears to be the case that we have\n",
    "# Sequences of varying length. May need\n",
    "# to train long seqs first.\n",
    "file = '/home/das-hund/PycharmProjects/autoencoders/data/pdb_seqres.txt'\n",
    "\n",
    "sequences = []\n",
    "iter_ = 0\n",
    "with open(file, 'r') as sequence_file:\n",
    "    for iter_, line in enumerate(sequence_file):\n",
    "        if (iter_ + 1) % 2 == 0:\n",
    "            sequences.append(line.strip())\n",
    "    \n",
    "sequence_count = len(sequences)\n",
    "print(f'[+] {sequence_count} Sequences in {file}')\n",
    "print(f'[+] Subset of Sequences:')\n",
    "print('\\n\\n'.join(sequences[:5]))\n",
    "print('\\n\\n'.join(sequences[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "After splitting the texts by new lines, we\n",
    "want to map the characters to integers. Note\n",
    "that we use the `char_level` argument to \n",
    "tokenize characters instead of words. \n",
    "\n",
    "Fortunately, we don't need to limit the \n",
    "number of tokens. There aren't many amino\n",
    "acids available. Worth noting, the last \n",
    "layer is currently one hot encoded. This\n",
    "is **absolutely** worth optimizing; \n",
    "however, time is a factor. \n",
    "\n",
    "We will also pad the sequences. To start,\n",
    "we also won't be limiting sequence length.\n",
    "I'll do some simple analysis to get a\n",
    "distribution of sequence length. This will\n",
    "give us an idea of whether it's worth\n",
    "limiting the length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {\n",
    "    index: word \n",
    "    for word, index in word_to_index.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = np.array([\n",
    "    len(seq) for seq in sequences\n",
    "])\n",
    "\n",
    "max_seq_len = max(sequence_lengths)\n",
    "word_count = len(word_to_index)\n",
    "\n",
    "print(f'[+] Max Sequence Length: {max_seq_len}')\n",
    "print(f'[+] Word Count: {word_count}')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.hist(sequence_lengths)\n",
    "\n",
    "# ToDo: Update the names of these params.\n",
    "MAX_WORDS = word_count\n",
    "MAX_SEQ_LEN = 256#max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'[+] {len(word_to_index)} words found in {file}')\n",
    "\n",
    "print('[+] Word Map Subset:\\n{')\n",
    "for acid, index in word_to_index.items():\n",
    "    print('\\t{}: {}'.format(acid, index))\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Subset\n",
    "\n",
    "When training the RNNS, we will need to limit the\n",
    "length of the data to a multiple of what our batch\n",
    "size will be. In the future, we will also perform \n",
    "the train test split here. But, for now, we just\n",
    "select a random subset of the data. This will serve\n",
    "as our development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEN = 486400\n",
    "np.random.shuffle(sequences)\n",
    "sequences = sequences[:DATA_LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_sequences[0:5])\n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_sequences = padded_sequences.reshape((512, 256, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note I'm not using the GLOVE embeddings here.\n",
    "That would make it difficult to transition this\n",
    "model to protein sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import ELU\n",
    "\n",
    "\n",
    "def build_encoder(encoder_input, max_seq_len, \n",
    "                  latent_dim, intermediate_dim,\n",
    "                  epsilon_std):\n",
    "    h = Bidirectional(LSTM(\n",
    "        intermediate_dim, return_sequences=True, name='lstm_encoding_one'\n",
    "    ), merge_mode='concat', name='bidirectional_encoding_one')(encoder_input)\n",
    "    h = Bidirectional(LSTM(\n",
    "        intermediate_dim // 2, return_sequences=False, name='lstm_encoding_two'\n",
    "    ), merge_mode='concat', name='bidirectional_encoding_two')(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean_, z_log_var_ = args\n",
    "        batch_size = K.shape(z_mean_)[0]\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "        return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "    \n",
    "    z_mean = Dense(latent_dim, activation='linear', name='z_mean')(h)\n",
    "    z_log_var = Dense(latent_dim, activation='linear', name='z_log_var')(h)\n",
    "    \n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        x = K.flatten(x)\n",
    "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "        xent_loss = max_seq_len * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "    latent = Lambda(\n",
    "        sampling, output_shape=(latent_dim,), name='latent'\n",
    "    )([z_mean, z_log_var])\n",
    "    \n",
    "    return vae_loss, latent\n",
    "\n",
    "\n",
    "def build_decoder(encoded_input, intermediate_dim,\n",
    "                  token_count, max_seq_len):\n",
    "    repeated_context = RepeatVector(\n",
    "        max_seq_len, name='repeated_context'\n",
    "    )(encoded_input)\n",
    "    \n",
    "    h = LSTM(\n",
    "        intermediate_dim // 2, return_sequences=True, name='lstm_decoding_one'\n",
    "    )(repeated_context)\n",
    "    h = LSTM(\n",
    "        intermediate_dim, return_sequences=True, name='lstm_decoding_two'\n",
    "    )(h)\n",
    "    \n",
    "    decoded = TimeDistributed(Dense(\n",
    "        token_count, activation='softmax', name='time_distributed_decoding'\n",
    "    ), name='decoded_mean')(h)\n",
    "    \n",
    "    return decoded\n",
    "    \n",
    "    \n",
    "def build_model(max_seq_len, embedding_dim, token_count,\n",
    "                batch_size, intermediate_dim, \n",
    "                latent_dim, epsilon_std=0.1):\n",
    "    # ENCODER\n",
    "    encoder_input = Input(shape=(max_seq_len, 1), name='encoder_input')\n",
    "    \n",
    "    vae_loss, encoded = build_encoder(\n",
    "        encoder_input=encoder_input, max_seq_len=max_seq_len,\n",
    "        latent_dim=latent_dim, intermediate_dim=intermediate_dim,\n",
    "        epsilon_std=epsilon_std\n",
    "    )\n",
    "    \n",
    "    encoder = Model(encoder_input, encoded, name='encoder')\n",
    "    \n",
    "    # DECODER\n",
    "    encoded_input = Input(shape=(latent_dim,), name='encoded_input')\n",
    "    decoded = build_decoder(\n",
    "        encoded_input=encoded_input, intermediate_dim=intermediate_dim,\n",
    "        token_count=token_count, max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    decoder = Model(encoded_input, decoded, name='decoder')\n",
    "    \n",
    "    # VAE\n",
    "    vae = Model(\n",
    "        encoder_input, \n",
    "        build_decoder(\n",
    "            encoded_input=encoded, intermediate_dim=intermediate_dim,\n",
    "            token_count=token_count, max_seq_len=max_seq_len\n",
    "        ), \n",
    "        name='vae')\n",
    "    vae.compile(\n",
    "        optimizer='adam',\n",
    "        loss=vae_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return vae, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = MAX_SEQ_LEN\n",
    "EMBED_DIM = 4\n",
    "WORD_COUNT = MAX_WORDS + 1\n",
    "BATCH_SIZE = 512\n",
    "STEPS_PER_EPOCH = DATA_LEN // BATCH_SIZE\n",
    "INTERMEDIATE_DIM = 128\n",
    "LATENT_DIM = 32\n",
    "EPOCHS = 6\n",
    "reload_model = True\n",
    "        \n",
    "vae, encoder, decoder = build_model(\n",
    "    max_seq_len=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    token_count=WORD_COUNT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    epsilon_std=0.1\n",
    ")\n",
    "\n",
    "if reload_model:\n",
    "    vae.load_weights('old_bestmodel.weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(vae, show_shapes=True, show_layer_names=True, to_file='vae.png')\n",
    "Image(retina=True, filename='vae.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_oh_encode(X_train, maxlen, num_words):\n",
    "    temp = np.zeros((X_train.shape[0], maxlen, num_words))\n",
    "    temp[\n",
    "        np.expand_dims(\n",
    "            np.arange(X_train.shape[0]), axis=0\n",
    "        ).reshape(\n",
    "            X_train.shape[0], 1\n",
    "        ), np.repeat(\n",
    "            np.array([np.arange(maxlen)]), X_train.shape[0], axis=0\n",
    "        ), X_train\n",
    "    ] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need this when we get the model working.\n",
    "def batch_generator(X, batch_size, max_seq_len, num_words):\n",
    "    indices = np.arange(len(X)) \n",
    "    batch=[]\n",
    "    while True:\n",
    "        np.random.shuffle(indices) \n",
    "        for i in indices:\n",
    "            batch.append(i)\n",
    "            if len(batch)==batch_size:\n",
    "                train = X[batch].reshape((batch_size, max_seq_len, 1))\n",
    "                yield train, complex_oh_encode(X[batch], maxlen=max_seq_len, num_words=num_words)\n",
    "                batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/das-hund/.conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/6\n",
      "572/950 [=================>............] - ETA: 7:28 - loss: 26.0177 - acc: 0.3823"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fcace2d40dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         )\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Add metrics for KL Div & Expectation\n",
    "fit_model = True\n",
    "fit_generator = True\n",
    "\n",
    "if fit_model:\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'old_bestmodel.weights.hdf5', monitor='loss',\n",
    "        verbose=1, save_best_only=True, mode='min'\n",
    "    )\n",
    "    ## change to monitor loss\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='loss', patience=2, \n",
    "        min_lr=0.0001, verbose=1\n",
    "    )\n",
    "    callbacks_list = [checkpoint, reduce_lr]\n",
    "    if fit_generator:\n",
    "        train_generator = batch_generator(\n",
    "            X=padded_sequences, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_seq_len=MAX_SEQUENCE_LENGTH,\n",
    "            num_words=WORD_COUNT\n",
    "        )\n",
    "        history = vae.fit_generator(\n",
    "            train_generator, \n",
    "            steps_per_epoch=STEPS_PER_EPOCH, \n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "    else:\n",
    "        trainX = padded_sequences.reshape(\n",
    "            (DATA_LEN, MAX_SEQ_LEN, 1)\n",
    "        )\n",
    "        trainY = complex_oh_encode(\n",
    "            padded_sequences, \n",
    "            maxlen=MAX_SEQ_LEN, \n",
    "            num_words=WORD_COUNT\n",
    "        )\n",
    "        history = model.fit(\n",
    "            trainX, trainY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks_list\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = padded_sequences.reshape((DATA_LEN, 256, 1))\n",
    "preds = vae.predict(predictors[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([\n",
    "    index_to_word[i] if i != 0 else ''\n",
    "    for i in padded_sequences[5]\n",
    "]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    pred_chars = [np.argmax(l) for l in preds[i]]\n",
    "    print(' '.join([\n",
    "        index_to_word[i] if i != 0 else '' \n",
    "        for i in pred_chars\n",
    "    ]).strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.save('vae.h5')\n",
    "encoder.save('encoder.h5')\n",
    "decoder.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
